# -*- coding: utf-8 -*-
"""
Created on Wed Apr 29 10:09:28 2020

@author: jingr

Model plotting

"""
import os, sys
sys.path.append('./libs')
sys.path.append('./tool/libs')
sys.path.append('../')



import os, sys, re
sys.path.append(os.path.curdir)
sys.path.append(sys.argv[0])

helpDoc = '''
The predicting script for using built model. 

Usage :
If using all the default parameters, users could use a simple command line after modeling:
    
    python tool/layerPlot.py --paraFile tmpOut/parameters.txt --outFigFolder tmpOut

The '--paraFile' was the parameters generated by using running.py, and the --outFigFolder is to specify the output folder, in which a file named UMAP.pdf will be saved.

To use a certain n_neighbor and min_dist, the command becomes:
    
    python tool/layerPlot.py --paraFile tmpOut/parameters.txt --outFigFolder tmpOut --n_neighbors 10 --min_dist 0.3

To use the grid search for the two parameters, the command becomes:
    
    python tool/layerPlot.py --paraFile tmpOut/parameters.txt --outFigFolder tmpOut --grid_min_dist 0.01 0.2 0.02 --grid_n_neighbors 2 30 2

The "0.01 0.2 0.02" and "2 30 2" above will be passed to arange() in numpy (i.e. used as "start stop stepSize"), where min_dist will be set as float and n_neighbors as int.
Note that if '--grid_min_dist' or '--grid_n_neighbors' used, '--min_dist' or '--n_neighbors' will be ignored respectively.

If grid search used, the plots will be save in a pdf with multiple pages.

Currently, the layer ranked at the last 2 will be used for plotting, users could use --layerIndex to change it. But please note that not all the layer is able to be used, please see the jupyter notebook for more details.

Another selection for specify the layer is using --interactive 1 and following the questions to decide the layer name or index.

To change the color, please use --theme or the --color_key_cmap and --background. NOTE that if --theme used, the --color_key_cmap and --background will not used for plotting.
options available for --theme:        
       * 'blue'
       * 'red'
       * 'green'
       * 'inferno'
       * 'fire'
       * 'viridis'
       * 'darkblue'
       * 'darkred'
       * 'darkgreen'
The parameter for --color_key_cmap is following matplotlib, please see https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html for details.
The parameter for --background could be a simple 'black' or 'white', but could be other color obeying the rule of matplotlib.

NOTE: This script is to plot the model layers using UMAP, currently only n_neighbor and min_dist are able to be modified, if users want to use more features of UMAP or integrate this script into other work, please use our jupyter notebook (in the folder "notebook") as a template.
'''
import paraParser
if '--help' in sys.argv:
    print(helpDoc)
    exit()

import moduleRead
import dataProcess
#import analysisPlot
import numpy as np
#from sklearn.metrics import accuracy_score,f1_score,roc_auc_score,recall_score,precision_score,confusion_matrix,matthews_corrcoef 
import tensorflow as tf
from utils import TextDecorate, evalStrList

from keras.models import Model
from keras.models import Sequential
import umap
import umap.plot
import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
td = TextDecorate()


defaultParaDict, numSet, intSet, boolSet, objSet = paraParser.getDefaultParameters()
defaultParaDict['grid_n_neighbors'] = []
defaultParaDict['grid_min_dist'] = []
defaultParaDict['n_neighbors'] = 15
defaultParaDict['min_dist'] = 0.1
defaultParaDict['layerIndex'] = -2
defaultParaDict['figWidth'] = 800
defaultParaDict['figHeight'] = 800
defaultParaDict['outFigFolder'] = None
defaultParaDict['metric'] = 'euclidean'
defaultParaDict['color_key_cmap'] = 'rainbow'
defaultParaDict['background'] = 'white'
defaultParaDict['theme'] = None
defaultParaDict['color_key_cmap'] = 'rainbow'
defaultParaDict['background'] = 'white'
defaultParaDict['interactive'] = False


#numSet.add('grid_min_dist')
#intSet.add('grid_n_neighbors')
numSet.add('min_dist')
intSet.add('n_neighbors')
intSet.add('layerIndex')
intSet.add('figWidth')
intSet.add('figHeight')
boolSet.add('interactive')

paraDictCMD = paraParser.parseParameters(sys.argv[1:],defaultParaTuple=(defaultParaDict, numSet, intSet, boolSet, objSet))
paraFile = paraDictCMD['paraFile']
paraDict = paraParser.parseParametersFromFile(paraFile,defaultParaTuple=(paraDictCMD, numSet, intSet, boolSet, objSet))

#paraFile = paraDictCMD['paraFile']
#if not paraFile is None:
#    paraDict = paraParser.parseParametersFromFile(paraFile)
#    paraDict['dataTestFilePaths'] = paraDictCMD['dataTestFilePaths']
#    paraDict['dataTestModelInd'] = paraDictCMD['dataTestModelInd']
#else:
#    paraDict = paraDictCMD.copy()


#paraFile = 'D:/workspace/autoBioSeqpy/tmpOut/parameters.txt'
#paraDict = paraParser.parseParametersFromFile(paraFile)
#paraDict['dataTestFilePaths'] = paraDictCMD['dataTestFilePaths']
#paraDict['dataTestModelInd'] = paraDictCMD['dataTestModelInd']
paraDict['dataTestFilePaths'] = paraDict['dataTrainFilePaths']
paraDict['dataTestModelInd'] = paraDict['dataTrainModelInd']
paraDict['dataTestLabel'] = paraDict['dataTrainLabel']



#parameters

grid_n_neighbors = paraDict['grid_n_neighbors']
if len(grid_n_neighbors) > 0:
    grid_n_neighbors = evalStrList(grid_n_neighbors)
grid_min_dist = paraDict['grid_min_dist']
if len(grid_min_dist) > 0:
    grid_min_dist = evalStrList(grid_min_dist)
layerIndex = paraDict['layerIndex']
figWidth = paraDict['figWidth']
figHeight = paraDict['figHeight']
outFigPath = paraDict['outFigFolder']
if outFigPath is None:
    outFigPath = paraDict['outSaveFolderPath']

#dataType = paraDict['dataType']
dataTypeList = paraDict['dataType']
dataEncodingType = paraDict['dataEncodingType']
spcLen = paraDict['spcLen']
firstKernelSize = paraDict['firstKernelSize']
#dataTrainFilePaths = paraDict['dataTrainFilePaths']
#dataTrainLabel = paraDict['dataTrainLabel']
dataTestFilePaths = paraDict['dataTestFilePaths']
dataTestLabel = paraDict['dataTestLabel']
#modelLoadFile = paraDict['modelLoadFile']
#weightLoadFile = paraDict['weightLoadFile']
dataSplitScale = paraDict['dataSplitScale']
outSaveFolderPath = paraDict['outSaveFolderPath']
showFig = paraDict['showFig']
saveFig = paraDict['saveFig']
savePrediction = paraDict['savePrediction']

interactive = paraDict['interactive']

loss = paraDict['loss']
optimizer = paraDict['optimizer']
if not optimizer.startswith('optimizers.'):
    optimizer = 'optimizers.' + optimizer
if not optimizer.endswith('()'):
    optimizer = optimizer + '()'
metrics = paraDict['metrics']

shuffleDataTrain = paraDict['shuffleDataTrain']
shuffleDataTest = paraDict['shuffleDataTest']
batch_size = paraDict['batch_size']
epochs = paraDict['epochs']

#useKMer = paraDict['useKMer']
#KMerNum = paraDict['KMerNum']
inputLength = paraDict['inputLength']
modelSaveName = paraDict['modelSaveName']
weightSaveName = paraDict['weightSaveName']
noGPU = paraDict['noGPU']
labelToMat = paraDict['labelToMat']


modelLoadFile = paraDict['modelLoadFile']
useKMerList = paraDict['useKMer']
if len(useKMerList ) == 0:
    useKMerList = [False] * len(modelLoadFile)
KMerNumList = paraDict['KMerNum']
if len(KMerNumList ) == 0:
    KMerNumList = [3] * len(modelLoadFile)
    
#dataTrainModelInd = paraDict['dataTrainModelInd']
#if len(modelLoadFile) == 1:
#    dataTrainModelInd = [0] * len(dataTrainFilePaths)
dataTestModelInd = paraDict['dataTestModelInd']
if len(modelLoadFile) == 1:
    dataTestModelInd = [0] * len(dataTestFilePaths)

modelPredictFile = outSaveFolderPath + os.path.sep + modelSaveName
weightLoadFile = outSaveFolderPath + os.path.sep + weightSaveName
#modelPredictFile = 'D:/workspace/autoBioSeqpy/tmpOut/tmpMod.json'
#weightLoadFile = 'D:/workspace/autoBioSeqpy/tmpOut/tmpWeight.bin'

if not os.path.exists(modelPredictFile):
    if os.path.exists('../'+modelPredictFile):
        os.chdir('../')
    else:
        td.printC('the model load file %s is unavailable, please check the path.' %modelPredictFile,'r')

model = moduleRead.readModelFromJsonFileDirectly(modelPredictFile,weightLoadFile)

    
#modelPredictFile = outSaveFolderPath + os.path.sep + modelSaveName
#
#
#
#weightLoadFile = outSaveFolderPath + os.path.sep + weightSaveName


def unBoundLayers(modelIn,layers = []):
    for layer in modelIn.layers:
        if not 'sequential' in layer.name.lower():
            layers.append(layer)
        else:
            unBoundLayers(layer,layers)
    return layers

def generateNewModelFromLayers(layers):
    newModel = Sequential()
    for layer in layers:
        newModel.add(layer)
    return newModel

def genrerateNewModelFromModel(oriModel, selectedLayerIndex = -2, td = td, interactive=False):
    isConcatenate, hasSequential, avaiLayerIndex = analAvaiLayers(oriModel)
    # print(selectedLayerIndex)
    layerIndexFix = selectedLayerIndex
    if layerIndexFix < 0:
        layerIndexFix = layerIndexFix + len(avaiLayerIndex)
    if layerIndexFix < 0 or layerIndexFix > len(avaiLayerIndex) - 1:
        td.printC('Only %d layers could be used to generating UMAP, but the index %d is out of the range.' %(len(analAvaiLayers),selectedLayerIndex),'r')
    
    if interactive:
        td.printC('Since interactive is set as True, few operations will be decided by users:','b')
        if hasSequential:
            td.printC('A sequential model detected in the current model, the index model is suggested.','p')
        useName = input('Using index (e.g. an integer such as 0,1,2,...) or the name printed above?\n0 for index and 1 for name:')
        while not (useName == '0' or useName == '1'):
            useName = input('type 0 or 1:')
        if useName == '1':
            td.printC('The names of the layers:','b')
            for layer in oriModel.layers:
                td.printC(layer.name,'B')
            layerName = input('Please provide the layer name:')
            newModel = Model(inputs=oriModel.input,outputs=oriModel.get_layer(layerName).output)
        else:
            td.printC('The names and indexes of the layers:','b')
            upackedLayers = unBoundLayers(oriModel)
            for i,layer in enumerate(upackedLayers):
                td.printC('%d: %s' %(i,layer.name), 'B')
            layerIndex = input('Please provide the layer index:')
            layerIndex = int(layerIndex)
            newModel = generateNewModelFromLayers(upackedLayers[:layerIndex+1])
    else:
        if isConcatenate:
            newModel = Model(inputs=oriModel.input,outputs=oriModel.layers[avaiLayerIndex[selectedLayerIndex]].output)
        else:
            if hasSequential:
                upackedLayers = unBoundLayers(oriModel)
                newModel = generateNewModelFromLayers(upackedLayers[:selectedLayerIndex+1])
            else:
                newModel=Model(inputs=oriModel.input,outputs=oriModel.layers[avaiLayerIndex[selectedLayerIndex]].output)
    return newModel

def analAvaiLayers(modelIn):
    isConcatenate = False
    hasSequential = False
    lastConcatenateLayerNum = None
    for i,layer in enumerate(modelIn.layers):
        if 'concatenate' in layer.name.lower():
            isConcatenate = True
            lastConcatenateLayerNum = i
        if 'sequential' in layer.name.lower():
            hasSequential = True
    avaiLayerIndex = []
    if isConcatenate:
        avaiLayerIndex = range(lastConcatenateLayerNum,len(modelIn.layers))
    else:
        avaiLayerIndex = range(len(modelIn.layers))
    return isConcatenate, hasSequential, list(avaiLayerIndex)

def plotOneUMAP(outName, testLabelArr, plotDict, featureDict = None, pdf=None, td=td):
    if featureDict is None:
        mapper = umap.UMAP().fit(predicted_Probability)
    else:
        mapper = umap.UMAP(**featureDict).fit(predicted_Probability)
#    fig = plt.figure()
    # print(testLabelArr)
    plotObj = umap.plot.points(mapper, labels=testLabelArr, **plotDict)
#    plt.savefig('tmpOut/tmp.jpg')
    
#    print('Saving %s' %(subTitle))
    if pdf is None:        
        plt.savefig(outName)
        td.printC('%s saved.' %(outName), 'g')
    else:
        subTitle = re.findall('(NNeighbor.+)\.pdf',outName)[0]
        plt.title(subTitle)
#        pdf.savefig(fig)
        pdf.savefig()
        td.printC('%s plotted.' %(subTitle), 'b')

    plt.clf()
    plt.close('all')
    
    
    
    
verbose = paraDict['verbose']

predictionSavePath = None
for i,k in enumerate(sys.argv):
    if k == '--predictionSavePath':
        predictionSavePath = sys.argv[i+1]
    elif k == '--verbose':
        verbose = sys.argv[i+1]

colorText = paraDict['colorText']
if colorText.lower() == 'auto':
    import platform
    if 'win' in platform.system().lower():
        td.disable()
elif not bool(eval(colorText)):
    td.disable()
    
if noGPU:
    if verbose:
        td.printC('As set by user, gpu will be disabled.','g')
    os.environ["CUDA_VISIBLE_DEVICES"] = '-1'
else:
    #check the version of tensorflow before configuration
    tfVersion = tf.__version__
    if int(tfVersion.split('.')[0]) >= 2:
#        config = tf.compat.v1.ConfigProto(allow_growth=True)
        config = tf.compat.v1.ConfigProto()
        config.gpu_options.allow_growth = True

        sess =tf.compat.v1.Session(config=config)
    else:
        config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))
        sess = tf.Session(config=config)


if not len(dataTypeList) == len(modelLoadFile):
    if verbose:
        td.printC('Please provide enough data type(s) as the number of --modelLoadFile','r')
assert len(dataTypeList) == len(modelLoadFile)

featureGenerators = []
for i,subDataType in enumerate(dataTypeList):
    if subDataType.lower() == 'protein':
        if verbose:
            td.printC('Enconding protein data for model %d ...' %i,'b')
        featureGenerator = dataProcess.ProteinFeatureGenerator(dataEncodingType[i], useKMer=useKMerList[i], KMerNum=KMerNumList[i])
    elif subDataType.lower() == 'dna':
        if verbose:
            td.printC('Enconding DNA data for model %d ...' %i,'b')
        featureGenerator = dataProcess.DNAFeatureGenerator(dataEncodingType[i], useKMer=useKMerList[i], KMerNum=KMerNumList[i])
    elif subDataType.lower() == 'rna':
        if verbose:
            td.printC('Enconding RNA data for model %d ...' %i,'b')
        featureGenerator = dataProcess.RNAFeatureGenerator(dataEncodingType[i], useKMer=useKMerList[i], KMerNum=KMerNumList[i])
    elif subDataType.lower() == 'other':
        if verbose:
            td.printC('Reading CSV-like data for model %d ...' %i,'b')
        featureGenerator = dataProcess.OtherFeatureGenerator()
    else:
        td.printC('Unknow dataType %r, please use \'protein\', \'dna\' ,\'rna\' or \'other\'' %subDataType, 'r')
    featureGenerators.append(featureGenerator)
    assert subDataType.lower() in ['protein','dna','rna','other']

#if dataType is None:
#    if verbose:
#        print('NO data type provided, please provide a data type suce as \'protein\', \'dna\' or\'rna\'')
#assert not dataType is None
#
#if dataType.lower() == 'protein':
#    if verbose:
#        print('Enconding protein data...')
#    featureGenerator = dataProcess.ProteinFeatureGenerator(dataEncodingType, useKMer=useKMer, KMerNum=KMerNum)
#elif dataType.lower() == 'dna':
#    if verbose:
#        print('Enconding DNA data...')
#    featureGenerator = dataProcess.DNAFeatureGenerator(dataEncodingType, useKMer=useKMer, KMerNum=KMerNum)
#elif dataType.lower() == 'rna':
#    if verbose:
#        print('Enconding RNA data...')
#    featureGenerator = dataProcess.RNAFeatureGenerator(dataEncodingType, useKMer=useKMer, KMerNum=KMerNum)
#else:
#    print('Unknow dataType %r, please use \'protein\', \'dna\' or\'rna\'' %dataType)
#assert dataType.lower() in ['protein','dna','rna']




if verbose:
    td.printC('Checking the number of test files, which should be larger than 1 (e.g. at least two labels)...','b')
assert len(dataTestFilePaths) > 0

if verbose:
    td.printC('Begin to generate test dataset...','b')

testDataLoadDict = {}    
for modelIndex in range(len(modelLoadFile)):
    testDataLoadDict[modelIndex] = []
#    testDataLoaders = []
for i,dataPath in enumerate(dataTestFilePaths):
    modelIndex = dataTestModelInd[i]
    featureGenerator = featureGenerators[modelIndex]
    dataLoader = dataProcess.DataLoader(label = dataTestLabel[i], featureGenerator=featureGenerator)
    dataLoader.readFile(dataPath, spcLen = spcLen[modelIndex])
    testDataLoadDict[modelIndex].append(dataLoader)

testDataMats = []
testLabelArrs = []
testNameLists = []
for modelIndex in range(len(modelLoadFile)):
    testDataLoaders = testDataLoadDict[modelIndex]
    testDataSetCreator = dataProcess.DataSetCreator(testDataLoaders)
    testDataMat, testLabelArr, nameList = testDataSetCreator.getDataSet(toShuffle=False, withNameList=True)
    testDataMats.append(testDataMat)
    testLabelArrs.append(testLabelArr)
    testNameLists.append(nameList)
if verbose:
    td.printC('Test datasets generated.','g')
nameTemp = testNameLists[0]    
testDataMats, testLabelArrs, sortedIndexes = dataProcess.matAlignByName(testDataMats,nameTemp,testLabelArrs,testNameLists)
testNameLists = [nameTemp] * len(testNameLists)
    
tmpTempLabel = testLabelArrs[0]
for tmpLabel in testLabelArrs:
    assert np.sum(np.array(tmpTempLabel) - np.array(tmpLabel)) == 0   
#
#    
#testDataLoaders = []
#for i,dataPath in enumerate(dataTestFilePaths):
#    #The label is set to 0, since we do not need the label for testing (only for accuracy calculating)
#    dataLoader = dataProcess.DataLoader(label = 0, featureGenerator=featureGenerator)
#    dataLoader.readFile(dataPath, spcLen = spcLen)
#    testDataLoaders.append(dataLoader)
#testDataSetCreator = dataProcess.DataSetCreator(testDataLoaders)
#testDataMat, testLabelArr = testDataSetCreator.getDataSet(toShuffle=shuffleDataTest)

if labelToMat:
    if verbose:
        td.printC('Since labelToMat is set, the labels would be changed to onehot-like matrix','g')
    testLabelArr,testLabelArrDict,testArrLabelDict = dataProcess.labelToMat(testLabelArrs[0])
#    print(testLabelArr)
else:
    testLabelArr = testLabelArrs[0]


    
if verbose:
#    print('Datasets generated, the scales are:\n\ttraining: %d x %d\n\ttest: %d x %d' %(trainDataMat.shape[0],trainDataMat.shape[1],testDataMat.shape[0],testDataMat.shape[1]))    
    td.printC('begin to prepare model...','b')
#    print('Loading keras model from .py files...')
    

#
#if not inputLength is None:
#    if inputLength == 0:
#        inputLength = testDataMat.shape[1]



if verbose:
    td.printC('Checking module file for modeling','b')
if modelPredictFile is None:
    if verbose:
        td.printC('please provide a model file in a json file.','r')
if weightLoadFile is None:
    if verbose:
        td.printC('the weight file is necessary for predicting, otherwise the model will be with initialized weight','r')
assert not modelPredictFile is None
assert not weightLoadFile is None

if verbose:
    td.printC('Loading module and weight file','b')
model = moduleRead.readModelFromJsonFileDirectly(modelPredictFile,weightLoadFile)
if verbose:
    td.printC('Module loaded, generating the summary of the module','b')
    model.summary()
#    
#if '2D' in str(model.layers[0].__class__):
#    if verbose:
#        print('2D layer detected, data will be reshaped accroding to the \'spcLen\'')    
#    if useKMer:
#        reshapeLen = spcLen - KMerNum + 1
#    else:            
#        reshapeLen = spcLen 
#    #newShape = (int(trainDataMat.shape[1]/spcLen),spcLen)
##    newShape = (int(trainDataMat.shape[1]/reshapeLen),reshapeLen)
##    trainDataMat = trainDataMat.reshape(trainDataMat.shape[0],int(trainDataMat.shape[1]/reshapeLen),reshapeLen,1)
#    testDataMat = testDataMat.reshape(testDataMat.shape[0],int(testDataMat.shape[1]/reshapeLen),reshapeLen,1)
#    
'''
if not outSaveFolderPath is None:
    if not os.path.exists(outSaveFolderPath):        
        os.makedirs(outSaveFolderPath, exist_ok=True)
    else:
        if verbose:
            td.printC('outpath %s is exists, the outputs might be overwirten' %outSaveFolderPath,'p')


predicted_Probability = model.predict(testDataMats)
if not 'predict_classes' in dir(model):
    prediction = np.rint(predicted_Probability)
#    if labelToMat:
#        prediction = dataProcess.matToLabel(np.array(prediction,dtype=int), testArrLabelDict,td=td)
else:
    prediction = model.predict_classes(testDataMats)
'''
#############################################################################################################
#############################################################################################################
#############################################################################################################
#############################################################################################################
#############################################################################################################
#############################################################################################################
if not outFigPath is None:
    if not os.path.exists(outFigPath):        
        os.makedirs(outFigPath, exist_ok=True)
#    else:
#        if verbose:
#            td.printC('outpath %s is exists, the outputs might be overwirten' %outSaveFolderPath,'p')    

#exclude reshape
#subModel = model.layers[1].layers[3]
#subModel.output

#tmpLayer = model.get_input_at(0)
#subModel=Model(inputs=model.input,outputs=model.layers[-2].output)
#subModel1 = Model(inputs=model.layers[1].input,outputs = subModel.layers[1].output)

#tmpModel =  Sequential()
#tmpModel.add(model.layers[0])
#tmpModel.add(model.layers[1].layers[0])
#tmpModel.add(model.layers[1].layers[1])
#tmpModel.add(model.layers[1].layers[2])
#tmpModel.add(model.layers[1].layers[3])
#tmpModel.add(model.layers[1].layers[4])
#tmpModel.add(model.layers[1].layers[5])
#tmpModel.add(model.layers[1].layers[6])
    

#upackedLayers = unBoundLayers(model)
#upackedModel = generateNewModelFromLayers(upackedLayers[:-4])
#upackedModel.summary()
oriPrediction = model.predict(testDataMats)
newModel = genrerateNewModelFromModel(model,selectedLayerIndex=layerIndex, td=td, interactive=interactive)
td.printC('The model for plotting is:','b')
newModel.summary()
predicted_Probability = newModel.predict(testDataMats)

if len(grid_min_dist) > 0 or len(grid_n_neighbors) > 0:
    pdf = PdfPages('%s/UMAP.pdf' %outFigPath)
    
featureDict={
        'n_neighbors' : None,
        'min_dist' : None,
        'metric' : defaultParaDict['metric'],
        }

plotDict = {}
if not paraDict['theme'] is None:
    plotDict['theme'] = paraDict['theme']
else:
    plotDict['color_key_cmap'] = paraDict['color_key_cmap']
    plotDict['background'] = paraDict['background']

#print(grid_n_neighbors)
if verbose:
    td.printC('Started to generating UMAP...', 'b')
    
if len(testLabelArr.shape) == 2:
    testLabelArr = np.argmax(testLabelArr, axis=-1)

if len(grid_min_dist) > 0 and len(grid_n_neighbors) > 0:
    for min_dist in np.arange(*grid_min_dist):
        featureDict['min_dist'] = min_dist
        for n_neighbors in np.arange(*grid_n_neighbors).astype(int):
            featureDict['n_neighbors'] = n_neighbors
            outName = '%s/UMAP_NNeighbor_%s_MDist_%s.pdf' %(outFigPath,str(n_neighbors),str(min_dist))
#            print(outName)
            plotOneUMAP(outName, testLabelArr, plotDict, featureDict = featureDict, pdf=pdf, td=td)
elif len(grid_min_dist) > 0:
    n_neighbors = paraDict['n_neighbors']
    featureDict['n_neighbors'] = n_neighbors
    for min_dist in np.arange(*grid_min_dist):
        featureDict['min_dist'] = min_dist
        outName = '%s/UMAP_NNeighbor_%s_MDist_%s.pdf' %(outFigPath,str(n_neighbors),str(min_dist))
        plotOneUMAP(outName, testLabelArr, plotDict, featureDict = featureDict, pdf=pdf, td=td)
elif len(grid_n_neighbors) > 0:
    min_dist = paraDict['min_dist']
    featureDict['min_dist'] = min_dist
    for n_neighbors in np.arange(*grid_n_neighbors).astype(int):
        featureDict['n_neighbors'] = n_neighbors
        outName = '%s/UMAP_NNeighbor_%s_MDist_%s.pdf' %(outFigPath,str(n_neighbors),str(min_dist))
        plotOneUMAP(outName, testLabelArr, plotDict, featureDict = featureDict, pdf=pdf, td=td)
else:
    n_neighbors = paraDict['n_neighbors']
    featureDict['n_neighbors'] = n_neighbors
    min_dist = paraDict['min_dist']
    featureDict['min_dist'] = min_dist
    outName = '%s/UMAP.pdf' %(outFigPath)
    plotOneUMAP(outName, testLabelArr, plotDict, featureDict = featureDict, td=td)
#mapper = umap.UMAP(**featureDict).fit(predicted_Probability)
#plt.figure()
#plotObj = umap.plot.points(mapper, labels=testLabelArr)
#plt.savefig('tmpOut/tmp.jpg')
#plotOneUMAP(featureDict = featureDict)


if len(grid_min_dist) > 0 or len(grid_n_neighbors) > 0:
    pdf.close()
    if verbose:
        td.printC('%s/UMAP.pdf saved.' %outFigPath, 'g')
